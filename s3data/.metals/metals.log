[0m2021.02.27 13:22:19 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data' for client vscode 1.53.2.[0m
[0m2021.02.27 13:22:20 INFO  time: initialize in 0.4s[0m
[0m2021.02.27 13:22:20 WARN  Build server is not auto-connectable.[0m
[0m2021.02.27 13:22:20 WARN  no build tool detected in workspace '/home/skyler/project3/s3data'. The most common cause for this problem is that the editor was opened in the wrong working directory, for example if you use sbt then the workspace directory should contain build.sbt. [0m
[0m2021.02.27 13:22:20 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 13:22:21 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    val parsedRDD = rdd
      .flatMap(line =>
        line.split("""\s+""") match {
          case Array(href, _) => Some(href)
        }
      )

    parsedRDD.take(100).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:22:23 INFO  time: code lens generation in 2.37s[0m
[0m2021.02.27 13:22:50 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(100).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:12 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(100).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(200).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:19 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(100).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:39 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(200).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:48 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 13:24:52 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(200).foreach(println)
    pars
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:53 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    parsedRDD.take(200).foreach(println)
    parsed
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:24:58 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rss.take(200).foreach(println)
    parsed
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:25:00 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    parsed
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:25:08 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:25:28 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:28 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:33 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:34 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:34 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:34 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Feb 27, 2021 1:25:35 PM scala.meta.internal.pc.CompilerAccess handleError
SEVERE: file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
file%3A%2F%2F%2Fhome%2Fskyler%2Fproject3%2Fs3data%2Fs3dataget%2Fsrc%2Fmain%2Fscala%2Fcom.revature.scala%2FGetS3Data.scala:57: error: identifier expected but } found
  }
  ^
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:16)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.Reporter.syntaxError(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter.syntaxError$(Reporter.scala:17)
	at scala.meta.internal.parsers.Reporter$$anon$1.syntaxError(Reporter.scala:22)
	at scala.meta.internal.parsers.ScalametaParser.syntaxErrorExpected(ScalametaParser.scala:835)
	at scala.meta.internal.parsers.ScalametaParser.name(ScalametaParser.scala:1829)
	at scala.meta.internal.parsers.ScalametaParser.termName(ScalametaParser.scala:1832)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$selector$1(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.selector(ScalametaParser.scala:1904)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExprRest$1(ScalametaParser.scala:2890)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.simpleExprRest(ScalametaParser.scala:2877)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$5(ScalametaParser.scala:2823)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2823)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockStatSeq$2(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.stat(ScalametaParser.scala:4787)
	at scala.meta.internal.parsers.ScalametaParser.blockStatSeq(ScalametaParser.scala:4944)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$block$1(ScalametaParser.scala:3002)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.block(ScalametaParser.scala:2995)
	at scala.meta.internal.parsers.ScalametaParser.blockOrCase$1(ScalametaParser.scala:2983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$3(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$blockExpr$1(ScalametaParser.scala:2986)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.blockExpr(ScalametaParser.scala:2979)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$simpleExpr0$1(ScalametaParser.scala:2802)
	at scala.meta.internal.parsers.ScalametaParser.atPosTry(ScalametaParser.scala:809)
	at scala.meta.internal.parsers.ScalametaParser.autoPosTry(ScalametaParser.scala:819)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr0(ScalametaParser.scala:2772)
	at scala.meta.internal.parsers.ScalametaParser.simpleExpr(ScalametaParser.scala:2770)
	at scala.meta.internal.parsers.ScalametaParser.prefixExpr(ScalametaParser.scala:2754)
	at scala.meta.internal.parsers.ScalametaParser.postfixExpr(ScalametaParser.scala:2734)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$14(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$expr$2(ScalametaParser.scala:2311)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2197)
	at scala.meta.internal.parsers.ScalametaParser.expr(ScalametaParser.scala:2092)
	at scala.meta.internal.parsers.ScalametaParser.exprMaybeIndented(ScalametaParser.scala:2120)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$funDefRest$1(ScalametaParser.scala:4236)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.funDefRest(ScalametaParser.scala:4178)
	at scala.meta.internal.parsers.ScalametaParser.funDefOrDclOrExtensionOrSecondaryCtor(ScalametaParser.scala:4107)
	at scala.meta.internal.parsers.ScalametaParser.defOrDclOrSecondaryCtor(ScalametaParser.scala:3963)
	at scala.meta.internal.parsers.ScalametaParser.nonLocalDefOrDcl(ScalametaParser.scala:3948)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4868)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$templateStat$1$1.applyOrElse(ScalametaParser.scala:4862)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.templateStats(ScalametaParser.scala:4878)
	at scala.meta.internal.parsers.ScalametaParser.templateStatSeq(ScalametaParser.scala:4858)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$templateBody$1(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.inBraces(ScalametaParser.scala:712)
	at scala.meta.internal.parsers.ScalametaParser.templateBody(ScalametaParser.scala:4707)
	at scala.meta.internal.parsers.ScalametaParser.templateBodyOpt(ScalametaParser.scala:4715)
	at scala.meta.internal.parsers.ScalametaParser.templateOpt(ScalametaParser.scala:4700)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$objectDef$1(ScalametaParser.scala:4439)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.objectDef(ScalametaParser.scala:4431)
	at scala.meta.internal.parsers.ScalametaParser.tmplDef(ScalametaParser.scala:4315)
	at scala.meta.internal.parsers.ScalametaParser.topLevelTmplDef(ScalametaParser.scala:4297)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4828)
	at scala.meta.internal.parsers.ScalametaParser$$anonfun$topStat$1.applyOrElse(ScalametaParser.scala:4816)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.meta.internal.parsers.ScalametaParser.statSeq(ScalametaParser.scala:4806)
	at scala.meta.internal.parsers.ScalametaParser.topStatSeq(ScalametaParser.scala:4815)
	at scala.meta.internal.parsers.ScalametaParser.bracelessPackageStats$1(ScalametaParser.scala:5021)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.semanticdb.scalac.ParseOps$XtensionCompilationUnitSource.toSource(ParseOps.scala:17)
	at scala.meta.internal.semanticdb.scalac.TextDocumentOps$XtensionCompilationUnitDocument.toTextDocument(TextDocumentOps.scala:201)
	at scala.meta.internal.pc.SemanticdbTextDocumentProvider.textDocument(SemanticdbTextDocumentProvider.scala:52)

[0m2021.02.27 13:25:36 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println)
    rdd.saveAs
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:25:41 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveA
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:25:42 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAs
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:11 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:13 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile()
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:26 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile(rdd)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:29 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile()
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:31 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile("rdd")
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:40 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4946264711272453352/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.27 13:26:40 WARN  no build target for: /home/skyler/project3/s3data/project/metals.sbt[0m
[0m2021.02.27 13:26:40 WARN  no build target for: /home/skyler/project3/s3data/project/project/metals.sbt[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.27 13:26:40 INFO  skipping build import with status 'Started'[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.27 13:26:41 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.02.27 13:26:43 INFO  [info] loading settings for project s3data-build-build from metals.sbt ...[0m
[0m2021.02.27 13:26:43 INFO  [info] loading project definition from /home/skyler/project3/s3data/project/project[0m
[0m2021.02.27 13:26:46 INFO  [info] loading settings for project s3data-build from metals.sbt ...[0m
[0m2021.02.27 13:26:46 INFO  [info] loading project definition from /home/skyler/project3/s3data/project[0m
[0m2021.02.27 13:26:48 INFO  [success] Generated .bloop/s3data-build.json[0m
[0m2021.02.27 13:26:48 INFO  [success] Total time: 2 s, completed Feb 27, 2021 1:26:48 PM[0m
[0m2021.02.27 13:26:48 INFO  [info] set current project to s3data (in build file:/home/skyler/project3/s3data/)[0m
[0m2021.02.27 13:26:48 INFO  [success] Generated .bloop/s3data.json[0m
[0m2021.02.27 13:26:48 INFO  [success] Generated .bloop/s3data-test.json[0m
[0m2021.02.27 13:26:48 INFO  [success] Total time: 0 s, completed Feb 27, 2021 1:26:48 PM[0m
[0m2021.02.27 13:26:49 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.27 13:26:49 INFO  time: ran 'sbt bloopInstall' in 9.55s[0m
[0m2021.02.27 13:26:49 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher9116841660338365819/bsp.socket'...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/.bloop/s3data-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/.bloop/s3data.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.12.
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/jansi.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/jline.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-compiler.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-library.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-reflect.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-xml_2.12.jar
[0m[32m[D][0m Missing analysis file for project 's3data'
[0m[32m[D][0m Missing analysis file for project 's3data-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher9116841660338365819/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher9116841660338365819/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 13:26:51 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 13:26:51 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1501249764422130344/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1501249764422130344/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1501249764422130344/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 13:26:52 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 13:26:52 INFO  time: Connected to build server in 2.56s[0m
[0m2021.02.27 13:26:52 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.27 13:26:52 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.27 13:26:54 INFO  time: indexed workspace in 1.87s[0m
[0m2021.02.27 13:26:54 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 13:26:54 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile("rdd")
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 13:26:57 INFO  time: code lens generation in 2.87s[0m
[0m2021.02.27 13:27:32 INFO  shutting down Metals[0m
[0m2021.02.27 13:27:32 INFO  Shut down connection with build server.[0m
[0m2021.02.27 13:27:32 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.28 15:15:10 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data' for client vscode 1.53.2.[0m
[0m2021.02.28 15:15:11 INFO  time: initialize in 1.07s[0m
[0m2021.02.28 15:15:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.28 15:15:13 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2286760083755588388/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.28 15:15:13 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.28 15:15:17 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/.bloop/s3data.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/.bloop/s3data-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.12.
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/jansi.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/jline.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-compiler.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-library.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-reflect.jar
[0m[32m[D][0m   => /home/skyler/.sbt/boot/scala-2.12.12/lib/scala-xml_2.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 's3data', 's3data-test'
[0m[32m[D][0m Missing analysis file for project 's3data'
[0m[32m[D][0m Missing analysis file for project 's3data-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2286760083755588388/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2286760083755588388/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 15:15:20 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:15:21 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1463039068151258595/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1463039068151258595/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1463039068151258595/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 15:15:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 15:15:21 INFO  time: code lens generation in 7.67s[0m
[0m2021.02.28 15:15:21 INFO  time: Connected to build server in 8.27s[0m
[0m2021.02.28 15:15:21 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.28 15:15:21 INFO  time: Imported build in 0.25s[0m
[0m2021.02.28 15:15:22 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.28 15:15:22 INFO  time: indexed workspace in 1.85s[0m
[0m2021.02.28 15:15:22 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.28 15:15:24 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:15:31 INFO  time: code lens generation in 5.33s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:31:00 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:32:06 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:32:47 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter()

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:32:48 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:32:53 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:32:58 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")
      .count

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")
      .count

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:33:45 INFO  shutting down Metals[0m
[0m2021.02.28 15:33:45 INFO  Shut down connection with build server.[0m
[0m2021.02.28 15:33:45 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
